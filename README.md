# Spectral-Decoupled Knowledge Distillation for Heterophilic Graphs

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Enabling MLP to outperform GNN teachers on heterophilic graphs through spectral decomposition and positional encoding, achieving 1.4x faster inference without graph structure at test time.**

---

## ğŸ¯ Highlights

- **Beats SOTA Baseline**: 38.16% vs GloGNN++ 37.34% on Actor dataset
- **Graph-Free Inference**: No adjacency matrix needed at test time
- **1.44x Faster**: Reduced inference latency
- **2.88x Smaller**: Reduced model size

---

## ğŸ“Š Main Results

### Heterophilic Graph Performance (Actor Dataset)

| Method | Type | Accuracy | Graph at Inference |
|--------|------|----------|-------------------|
| GCN | GNN | 27.16% Â± 1.12% | Required |
| GAT | GNN | 27.16% Â± 1.12% | Required |
| GloGNN++ | GNN | 37.34% Â± 0.70% | Required |
| Vanilla MLP | MLP | 34.37% Â± 0.48% | Not needed |
| **Ours (Spectral KD)** | MLP | **38.16% Â± 1.05%** | **Not needed** |

### Efficiency Comparison

| Metric | GloGNN++ (Teacher) | Ours (Student) | Improvement |
|--------|-------------------|----------------|-------------|
| Parameters | 546K | 379K | 1.44x smaller |
| Model Size | 4.17 MB | 1.45 MB | 2.88x smaller |
| Inference Time | 46.95 ms | 32.58 ms | 1.44x faster |
| Requires Graph | Yes | **No** | âœ… |

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/your-repo/Spectral-KD-GNN.git
cd Spectral-KD-GNN
pip install -r requirements.txt
```

### Reproduce SOTA Results

```bash
# Step 1: Generate positional encoding
python features/generate_pe.py --dataset actor --k 16

# Step 2: Generate teacher logits (GloGNN++)
python baselines/save_teacher_logits.py --dataset actor --quick

# Step 3: Generate homophily weights
python features/generate_homophily.py --dataset actor --hard

# Step 4: Train with Spectral KD (reproduces 38.16%)
python train.py --dataset actor --num_runs 10
```

### One-Line Reproduction (if features already generated)

```bash
python train.py --dataset actor --num_runs 10 --epochs 300
```

---

## ğŸ”¬ Method Overview

### Key Innovation: Spectral-Decoupled Loss

We decompose teacher knowledge into **low-frequency** (smooth) and **high-frequency** (sharp) components:

```
L_spectral = h Ã— L_low + (1-h) Ã— L_high
```

Where:
- `L_low`: KL divergence on neighbor-averaged logits (captures global patterns)
- `L_high`: MSE on residual logits (captures local deviations)
- `h`: Per-node homophily weight (adaptive gating)

### Architecture

```
Input Features (932-dim) + RWPE (16-dim)
    â†“
LayerNorm â†’ Linear â†’ LayerNorm â†’ ReLU â†’ Dropout
    â†“
[Residual Block] Ã— 2
    â†“
Linear â†’ Output (5 classes)
```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ train.py                 # Main training script (SOTA entry point)
â”œâ”€â”€ run_ablation.py          # Ablation study experiments
â”œâ”€â”€ benchmark_efficiency.py  # Speed/memory benchmarks
â”‚
â”œâ”€â”€ models.py                # EnhancedMLP, ResMLP definitions
â”œâ”€â”€ layers.py                # Graph convolution layers
â”‚
â”œâ”€â”€ kd_losses/
â”‚   â”œâ”€â”€ adaptive_kd.py       # Spectral-Decoupled Loss (core contribution)
â”‚   â”œâ”€â”€ st.py                # Soft Target loss
â”‚   â””â”€â”€ rkd.py               # Relational KD loss
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ generate_pe.py       # Random Walk Positional Encoding
â”‚   â””â”€â”€ generate_homophily.py # Teacher-based homophily weights
â”‚
â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ run_glognn_baseline.py  # GloGNN++ implementation
â”‚   â””â”€â”€ save_teacher_logits.py  # Save teacher predictions
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ data_utils.py        # Dataset loading (Geom-GCN splits)
â”‚
â”œâ”€â”€ results/                 # Experiment results (JSON)
â””â”€â”€ figures/                 # Visualizations
```

---

## ğŸ“ˆ Ablation Study

| Variant | Model | PE | Loss | Accuracy |
|---------|-------|-----|------|----------|
| A | Plain MLP | âœ— | KL | 37.41% |
| B | Enhanced MLP | âœ“ | KL | 35.81% |
| **C** | Enhanced MLP | âœ“ | Spectral | **38.16%** |

**Key Finding**: Spectral Loss contributes +2.35% improvement. PE alone hurts without proper loss guidance.

---

## ğŸ”§ Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--hidden` | 256 | Hidden dimension |
| `--num_layers` | 3 | Number of MLP layers |
| `--lambda_spectral` | 1.0 | Spectral loss weight |
| `--lambda_soft` | 0.5 | Soft target loss weight |
| `--alpha_high` | 1.5 | High-frequency loss weight |
| `--temperature` | 4.0 | KD temperature |
| `--lr` | 0.01 | Learning rate |
| `--epochs` | 300 | Training epochs |

---

## ğŸ“š Requirements

```
torch>=1.10.0
torch_geometric>=2.0.0
numpy>=1.20.0
scipy>=1.7.0
tqdm>=4.60.0
```

---

## ğŸ“– Citation

If you find this work useful, please cite:

```bibtex
@article{spectral_kd_gnn,
  title={Spectral-Decoupled Knowledge Distillation for Heterophilic Graphs},
  author={Your Name},
  year={2024}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- GloGNN++ authors for the strong baseline
- PyTorch Geometric team for the excellent library
- Geom-GCN authors for standard heterophilic graph splits
