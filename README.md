# Spectral-Decoupled Knowledge Distillation for Heterophilic Graphs

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.10+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Enabling MLP to achieve competitive performance with GNN teachers on heterophilic graphs through spectral decomposition and positional encoding, achieving faster inference with no message passing at test time.**

---

## ğŸ¯ Highlights

- **Competitive with SOTA**: Achieves comparable or better accuracy than GloGNN++ on heterophilic graphs
- **No Message Passing at Inference**: Only requires pre-computed positional encoding; no graph convolution needed during inference
- **Faster Inference**: Reduced inference latency compared to GNN teachers
- **Smaller Model**: Reduced model size and memory footprint

> **Note on "Graph-Free" Claims**: While our method does not require message passing during inference, it does require pre-computed Random Walk Positional Encoding (RWPE) which uses the graph structure. The graph is only needed during the preprocessing stage, not during online inference. This is similar to how many efficient GNN methods pre-compute structural features.

---

## ğŸ“Š Phase 1: Evaluation Protocol (Completed âœ“)

We follow a rigorous evaluation protocol to ensure fair comparison:

1. **Data Splits**: GloGNN's official splits (Geom-GCN standard: 48%/32%/20%)
2. **10-Fold Evaluation**: All results are averaged over 10 fixed splits
3. **Teacher Verification**: GloGNN++ teacher reproduces original paper results within Â±2%
4. **Baseline Comparison**: We compare against GLNN (vanilla soft-label distillation)

### Teacher Verification Results

| Dataset | Paper | Reproduced | Diff | Status |
|---------|-------|------------|------|--------|
| Actor | 37.70% | 37.40% Â± 1.04% | -0.30% | âœ“ PASSED |
| Chameleon | 71.21% | 73.09% Â± 1.97% | +1.88% | âœ“ PASSED |
| Squirrel | 57.88% | 59.68% Â± 1.75% | +1.80% | âœ“ PASSED |

### Main Results (Heterophilic Graphs)

| Method | Type | Actor | Chameleon | Squirrel | Inference |
|--------|------|-------|-----------|----------|-----------|
| GloGNN++ (Teacher) | GNN | 37.40% | 73.09% | 59.68% | Message Passing |
| GLNN Baseline | MLP | 36.64% Â± 0.43% | 70.46% Â± 1.41% | 58.96% Â± 1.58% | No MP |
| Vanilla MLP | MLP | 33.91% Â± 0.78% | 45.66% Â± 1.48% | 30.29% Â± 1.85% | No MP |
| **Ours (Spectral KD)** | MLP | 36.62% Â± 1.27% | 70.75% Â± 2.57% | 57.23% Â± 2.22% | No MP* |

*Requires pre-computed RWPE (one-time preprocessing)

**Current Status**: Method needs improvement to consistently beat GLNN baseline.

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/your-repo/Spectral-KD-GNN.git
cd Spectral-KD-GNN
pip install -r requirements.txt

# Clone GloGNN for official splits and data
git clone https://github.com/RecklessRonan/GloGNN.git ../GloGNN
```

### Phase 1: Establish Evaluation Baseline (Completed)

```bash
# Run complete Phase 1 evaluation
python run_phase1_evaluation.py --all --device cuda

# Or run individual tasks:
# Step 1: Verify GloGNN++ teacher
python baselines/verify_glognn_teacher.py --all --device cuda

# Step 2: Run GLNN baseline
python baselines/glnn_baseline.py --all --device cuda
```

### Phase 2: Run Our Method

```bash
# Step 1: Generate positional encoding (pre-processing)
python features/generate_pe.py --dataset actor

# Step 2: Generate homophily weights
python features/generate_homophily.py --dataset actor --teacher_logits checkpoints/glognn_teacher_actor/split_0/teacher_logits.pt

# Step 3: Train with Spectral KD
python train.py --dataset actor --num_runs 10 --cuda
```

---

## ğŸ”¬ Method Overview

### Key Innovation: Spectral-Decoupled Loss

We decompose teacher knowledge into **low-frequency** (smooth) and **high-frequency** (sharp) components:

```
L_spectral = h Ã— L_low + (1-h) Ã— L_high
```

Where:
- `L_low`: KL divergence on neighbor-averaged logits (captures global patterns)
- `L_high`: MSE on residual logits (captures local deviations)
- `h`: Per-node homophily weight (adaptive gating)

### Why This Matters for Heterophilic Graphs

On heterophilic graphs, neighboring nodes often have different labels. Standard KD forces the student to mimic the teacher's smoothed predictions, which can be harmful. Our spectral decomposition:

1. **Separates smooth vs. sharp knowledge**: Low-frequency captures global class structure; high-frequency captures local deviations
2. **Adapts to local structure**: Nodes with low homophily emphasize high-frequency (sharp) knowledge
3. **Preserves discriminative information**: High-frequency residuals contain class-discriminative signals

### Architecture

```
Input Features + RWPE (pre-computed)
    â†“
LayerNorm â†’ Linear â†’ LayerNorm â†’ ReLU â†’ Dropout
    â†“
[Residual Block] Ã— 2
    â†“
Linear â†’ Output
```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiment_config.py  # Standardized evaluation settings
â”‚
â”œâ”€â”€ train.py                  # Main training script
â”œâ”€â”€ run_ablation.py           # Ablation study experiments
â”œâ”€â”€ benchmark_efficiency.py   # Speed/memory benchmarks
â”‚
â”œâ”€â”€ models.py                 # EnhancedMLP, ResMLP definitions
â”œâ”€â”€ layers.py                 # Graph convolution layers
â”‚
â”œâ”€â”€ kd_losses/
â”‚   â”œâ”€â”€ adaptive_kd.py        # Spectral-Decoupled Loss (core contribution)
â”‚   â”œâ”€â”€ st.py                 # Soft Target loss
â”‚   â””â”€â”€ rkd.py                # Relational KD loss
â”‚
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ generate_pe.py        # Random Walk Positional Encoding
â”‚   â””â”€â”€ generate_homophily.py # Teacher-based homophily weights
â”‚
â”œâ”€â”€ baselines/
â”‚   â”œâ”€â”€ verify_glognn_teacher.py  # Teacher verification (Task 2)
â”‚   â”œâ”€â”€ glnn_baseline.py          # GLNN baseline (Task 3)
â”‚   â””â”€â”€ run_glognn_baseline.py    # GloGNN++ implementation
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_utils.py         # Legacy data loading
â”‚   â””â”€â”€ data_loader_v2.py     # Standardized data loading (GloGNN splits)
â”‚
â”œâ”€â”€ results/                  # Experiment results (JSON)
â””â”€â”€ checkpoints/              # Saved models and teacher logits
```

---

## ğŸ“ˆ Ablation Study

| Variant | Model | PE | Loss | Accuracy | Î” |
|---------|-------|-----|------|----------|---|
| A | Plain MLP | âœ— | KL | TBD | baseline |
| B | Enhanced MLP | âœ“ | KL | TBD | TBD |
| **C** | Enhanced MLP | âœ“ | Spectral | TBD | TBD |

---

## ğŸ”§ Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--hidden` | 256 | Hidden dimension |
| `--num_layers` | 3 | Number of MLP layers |
| `--lambda_spectral` | 1.0 | Spectral loss weight |
| `--lambda_soft` | 0.5 | Soft target loss weight |
| `--alpha_high` | 1.5 | High-frequency loss weight |
| `--temperature` | 4.0 | KD temperature |
| `--lr` | 0.01 | Learning rate |
| `--epochs` | 500 | Training epochs |

---

## ğŸ“š Requirements

```
torch>=1.10.0
torch_geometric>=2.0.0
numpy>=1.20.0
scipy>=1.7.0
tqdm>=4.60.0
```

---

## ğŸ“– Citation

If you find this work useful, please cite:

```bibtex
@article{spectral_kd_gnn,
  title={Spectral-Decoupled Knowledge Distillation for Heterophilic Graphs},
  author={Your Name},
  year={2024}
}
```

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [GloGNN](https://github.com/RecklessRonan/GloGNN) authors for the strong baseline and official data splits
- [GLNN](https://github.com/snap-stanford/graphless-neural-networks) authors for the distillation framework
- PyTorch Geometric team for the excellent library
