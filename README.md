# Structure-Aware GNN Knowledge Distillation

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.4+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Transferring Graph Neural Network Knowledge to MLP with Topology-Aware Distillation**

This repository implements **Structure-Aware Knowledge Distillation** for Graph Neural Networks, enabling lightweight MLP models to achieve competitive (and sometimes superior!) performance compared to GNN teachers, without requiring graph structure during inference.

## ðŸŒŸ Highlights

- **Student Beats Teacher**: On Actor dataset, Student MLP outperforms Teacher GAT by **6.33%** (p < 0.001)
- **Statistical Significance**: 2 datasets show significant improvements with p < 0.01
- **4-10x Faster Inference**: MLP requires no graph structure at test time
- **Comprehensive Experiments**: 7 datasets covering both homophilic and heterophilic graphs

## ðŸ“Š Main Results

### Homophilic Graphs (GAT Teacher)

| Dataset | Teacher (GAT) | Student (MLP) | Gap | Significance |
|---------|---------------|---------------|-----|--------------|
| Cora | 82.74 Â± 0.74 | **82.99 Â± 1.22** | +0.25% | |
| Citeseer | 71.39 Â± 0.89 | 71.08 Â± 1.06 | -0.31% | |
| PubMed | 78.00 Â± 0.40 | **79.51 Â± 0.84** | +1.51% | *** |
| Amazon-Photo | 94.27 Â± 0.46 | **94.48 Â± 0.76** | +0.22% | |

### Heterophilic Graphs (GAT Teacher) - ðŸ”¥ Key Finding

| Dataset | Teacher (GAT) | Student (MLP) | Gap | Significance |
|---------|---------------|---------------|-----|--------------|
| Chameleon | **58.22 Â± 1.91** | 53.21 Â± 2.40 | -5.01% | |
| Squirrel | 33.15 Â± 1.27 | 32.88 Â± 1.49 | -0.28% | |
| **Actor** | 27.16 Â± 1.12 | **33.49 Â± 1.65** | **+6.33%** | **âœ¨ ***| |

> **Key Insight**: On heterophilic graphs with low average degree (Actor: 4.94), MLP's independence from noisy neighbor aggregation becomes advantageous!

### Statistical Significance (Paired t-test)

| Dataset | Gap | p-value | Result |
|---------|-----|---------|--------|
| **Actor** | +6.33% | < 0.001 | âœ… Significant |
| **PubMed** | +1.51% | 0.0003 | âœ… Significant |
| Cora | +0.25% | 0.377 | Not significant |
| Amazon-Photo | +0.22% | 0.451 | Not significant |

## ðŸ”¬ Method

### Loss Function

The distillation loss combines four components:

```
L_total = Î± Ã— L_task + Î² Ã— L_kd + Î³ Ã— L_rkd + Î» Ã— L_topo
```

| Loss | Description | Purpose |
|------|-------------|---------|
| L_task | CrossEntropy with ground truth | Learn correct labels |
| L_kd | KL divergence with soft labels (T=4.0) | Mimic teacher's predictions |
| L_rkd | Relational Knowledge Distillation | Preserve pairwise relationships |
| L_topo | Topology Consistency Loss | Align with graph structure |

### Innovation: Topology Consistency Distillation (TCD)

Unlike vanilla RKD which ignores graph structure, TCD explicitly aligns student's feature similarity with the graph adjacency:

```python
# Only compute loss for connected node pairs (edges)
student_sim = (student_feat[src] * student_feat[dst]).sum(dim=1)
teacher_sim = (teacher_feat[src] * teacher_feat[dst]).sum(dim=1)
loss_topo = MSE(student_sim, teacher_sim)
```

**Key Properties**:
- Edge-based computation: O(E) instead of O(NÂ²)
- Memory efficient: Uses sparse operations
- Transfers topological knowledge without requiring graph at inference

## ðŸš€ Quick Start

### Installation

```bash
git clone https://github.com/kai666-song/Structure-Aware-GNN-Distillation.git
cd Structure-Aware-GNN-Distillation
pip install -r requirements.txt
```

### Run Experiments

```bash
# 1. Baseline benchmark (all datasets)
python benchmark.py --all --num_runs 10

# 2. Main distillation with GAT teacher (recommended)
python distill_gat.py --data cora --num_runs 10

# 3. Heterophilic graph experiments (Actor, Squirrel, Chameleon)
python experiments_improved.py --experiment heterophilic --num_runs 10

# 4. Statistical significance tests
python experiments_improved.py --experiment significance_test

# 5. Citeseer optimization with degree-aware loss
python experiments_improved.py --experiment citeseer_optimize --num_runs 10
```

### Reproduce All Results

```bash
# Run complete experiment suite
python experiments_improved.py --experiment all --num_runs 10
```

## ðŸ“ Project Structure

```
â”œâ”€â”€ models.py                 # GCN, GAT, MLP, MLPBatchNorm definitions
â”œâ”€â”€ layers.py                 # Graph convolution layer
â”œâ”€â”€ distill_gat.py           # Main distillation script (GAT teacher)
â”œâ”€â”€ distill.py               # Distillation with GCN teacher
â”œâ”€â”€ experiments_improved.py   # Heterophilic + significance tests
â”œâ”€â”€ benchmark.py             # Baseline performance benchmark
â”œâ”€â”€ ablation_study.py        # Ablation experiments
â”‚
â”œâ”€â”€ kd_losses/               # Knowledge distillation losses
â”‚   â”œâ”€â”€ st.py               # Soft Target (KL divergence)
â”‚   â”œâ”€â”€ rkd.py              # Relational KD (pairwise similarity)
â”‚   â””â”€â”€ topology_kd.py      # Topology Consistency Loss (TCD)
â”‚
â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”œâ”€â”€ data_utils.py       # Dataset loading (Planetoid, Amazon, Heterophilic)
â”‚   â””â”€â”€ utils.py            # Helper functions
â”‚
â”œâ”€â”€ results/                 # Experiment results (JSON + Markdown)
â”œâ”€â”€ figures/                 # Visualizations (t-SNE, training curves)
â”œâ”€â”€ checkpoints/             # Saved model weights
â””â”€â”€ data/                    # Dataset files
```

## ðŸ“ˆ Ablation Study

### Effect of Structure Loss (Î³)

| Dataset | MLP Baseline | GLNN (Î³=0) | Ours (Î³=1) | Improvement |
|---------|--------------|------------|------------|-------------|
| Cora | 45.69 | 81.82 | **82.31** | +0.49% |
| Amazon-Computers | 41.25 | 81.47 | **83.15** | +1.68% |
| Amazon-Photo | 89.92 | 92.85 | **93.52** | +0.67% |

### Citeseer Optimization (Degree-Aware Loss)

| Config | Î»_topo | Min Degree | Accuracy |
|--------|--------|------------|----------|
| Baseline | 1.0 | - | 71.25 Â± 1.78 |
| Reduced | 0.3 | - | 71.06 Â± 1.68 |
| **Degree-Aware** | 0.5 | 2 | **71.33 Â± 1.31** |

## ðŸ”§ Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| Î± (alpha) | 1.0 | Task loss weight |
| Î² (beta) | 1.0 | KD loss weight |
| Î³ (gamma) | 1.0 | RKD loss weight |
| Î» (lambda_topo) | 1.0 | Topology loss weight |
| Temperature | 4.0 | Soft target temperature |
| Hidden dim | 64/256 | Hidden layer dimension |
| Dropout | 0.5 | Dropout rate |

## ðŸ“š Datasets

| Dataset | Nodes | Edges | Features | Classes | Type |
|---------|-------|-------|----------|---------|------|
| Cora | 2,708 | 5,429 | 1,433 | 7 | Homophilic |
| Citeseer | 3,327 | 4,732 | 3,703 | 6 | Homophilic |
| PubMed | 19,717 | 44,338 | 500 | 3 | Homophilic |
| Amazon-Photo | 7,650 | 119,081 | 745 | 8 | Homophilic |
| Chameleon | 2,277 | 36,101 | 2,325 | 5 | Heterophilic |
| Squirrel | 5,201 | 217,073 | 2,089 | 5 | Heterophilic |
| Actor | 7,600 | 33,544 | 932 | 5 | Heterophilic |

## ðŸ“– References

```bibtex
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{kipf2017semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={ICLR},
  year={2017}
}

@inproceedings{park2019relational,
  title={Relational knowledge distillation},
  author={Park, Wonpyo and Kim, Dongju and Lu, Yan and Cho, Minsu},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{velickovic2018graph,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and others},
  booktitle={ICLR},
  year={2018}
}
```

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

- PyTorch Geometric team for excellent graph learning library
- Original GCN and GAT authors for foundational work
- Knowledge distillation community for inspiring methods
